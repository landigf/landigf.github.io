<!doctype html>
<html lang="en" class="no-js">
<head>
  <meta charset="utf-8">
  
  <!-- begin SEO -->
  <title>PAI Course Notes: Probabilistic AI and Uncertainty</title>
  <meta property="og:locale" content="en-US">
  <meta property="og:site_name" content="Gennaro Francesco Landi">
  <meta property="og:title" content="PAI Course Notes: Probabilistic AI and Uncertainty">
  <meta property="og:description" content="Deep dive into Probabilistic Artificial Intelligence - exploring Bayesian approaches, uncertainty quantification, and intelligent agents that know what they don't know.">
  <meta property="og:url" content="https://landigf.github.io/pai-notes.html">
  <meta property="og:type" content="article">
  <meta property="article:author" content="Gennaro Francesco Landi">
  <meta property="article:published_time" content="2025-09-25T00:00:00+00:00">
  <meta name="description" content="Deep dive into Probabilistic Artificial Intelligence - exploring Bayesian approaches, uncertainty quantification, and intelligent agents that know what they don't know.">
  
  <script type="application/ld+json">
  {
    "@context" : "http://schema.org",
    "@type" : "Person",
    "name" : "Gennaro Francesco Landi",
    "url" : "https://landigf.github.io",
    "sameAs" : ["https://www.linkedin.com/in/landigf", "https://github.com/landigf"],
    "jobTitle" : "Master's Student in Computer Science",
    "affiliation" : "ETH Zurich"
  }
  </script>
  <!-- end SEO -->
  
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <script>
    document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  </script>
  
  <link rel="stylesheet" href="style.css">
  <link rel="icon" href="favicon-32x32.png" type="image/png">
  <meta http-equiv="cleartype" content="on">
  
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
  <!--[if lt IE 9]>
  <div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
  <![endif]-->
  
  <!-- Top Navigation Bar -->
  <nav class="top-nav">
    <div class="nav-container">
      <a href="index.html" class="nav-brand">Master's Student</a>
      <div class="nav-links">
        <a href="blog.html">Blog Posts</a>
        <a href="https://drive.google.com/file/d/1I1sEeulfPVFbF_8xYnmB6Xys3JaatR5r/view?usp=sharing" target="_blank">CV</a>
        <button id="theme-toggle" class="theme-toggle" title="Toggle dark mode">
          <i class="fas fa-moon"></i>
        </button>
      </div>
    </div>
  </nav>

  <!-- Main Content Container -->
  <div class="main-container">
    <!-- Left Sidebar with Profile -->
    <aside class="sidebar">
      <div class="profile-section">
        <div class="profile-image">
          <img src="profile_pic.png" alt="Gennaro Francesco Landi">
        </div>
        <div class="profile-info">
          <h2>Gennaro Francesco</h2>
          <div class="contact-info">
            <div class="contact-item">
              <i class="fas fa-map-marker-alt"></i>
              <span>Salerno, Italy</span>
            </div>
            <div class="contact-item">
              <i class="fas fa-envelope"></i>
              <a href="mailto:landig@student.ethz.ch">landig@student.ethz.ch</a>
            </div>
            <div class="contact-item">
              <i class="fab fa-linkedin"></i>
              <a href="https://www.linkedin.com/in/landigf" target="_blank">LinkedIn</a>
            </div>
            <div class="contact-item">
              <i class="fab fa-github"></i>
              <a href="https://github.com/landigf" target="_blank">GitHub</a>
            </div>
          </div>
        </div>
      </div>
    </aside>

    <!-- Main Content Area -->
    <main class="content">
      <div class="back-link">
        <a href="blog.html">‚Üê Back to Blog</a>
      </div>
      
      <article class="blog-post-full">
        <div class="post-header">
          <div class="post-meta">
            <span class="post-date">25 September 2025</span>
            <span class="post-category">Artificial Intelligence</span>
          </div>
          <h1>PAI Course Notes: Probabilistic AI and Uncertainty</h1>
        </div>
        
        <div class="post-content">
          <div class="course-info">
            <h3>Course Information</h3>
            <ul>
              <li><strong>Course:</strong> Probabilistic Artificial Intelligence (PAI)</li>
              <li><strong>Semester:</strong> Fall 2025</li>
              <li><strong>University:</strong> ETH Zurich</li>
              <li><strong>Status:</strong> <span class="status-badge">In Progress</span></li>
              <li><strong>üìÑ Course Materials:</strong> <a href="https://drive.google.com/file/d/13Lw92z64lMlmOWSaT3vdlNoBmnJIXIKp/view?usp=sharing" target="_blank">PDF Notes</a></li>
            </ul>
          </div>

          <blockquote class="key-definition">
            <strong>PAI Core Principle:</strong> A key aspect of intelligence is not to only make decisions, but reason about the <em>uncertainty</em> of these decisions, and to consider this uncertainty when making decisions. This is what PAI is about.
          </blockquote>

          <p>Welcome to my deep dive into <strong>Probabilistic Artificial Intelligence</strong>! This course is fundamentally changing how I think about machine learning and AI systems. It's not just about making machines <em>smart</em>, but about making them <em>humble</em>: systems that <strong>know what they don't know</strong>, and act cautiously when uncertainty is high.</p>

          <h2>Part I: Probabilistic Approaches to Machine Learning</h2>

          <p>The first part of the program covers probabilistic approaches to machine learning. We exploit the crucial difference between two types of uncertainty:</p>

          <ul>
            <li><strong>Epistemic uncertainty</strong> - due to lack of data (reducible)</li>
            <li><strong>Aleatoric uncertainty</strong> - irreducible noise from observations and outcomes</li>
          </ul>

          <p>Then we discuss concrete approaches toward probabilistic inference, including some fascinating methods:</p>

          <h3>1. Bayesian Linear Regression</h3>
          <p>Think of it as linear regression with a twist: instead of just one line through the data, we put a <strong>probability distribution over all possible lines</strong>, updating our belief as we see more points. The result is not just a prediction, but a quantified uncertainty around that prediction.</p>

          <h3>2. Gaussian Process Models</h3>
          <p>A Gaussian process is like saying: <em>"I don't know the exact function, but I'll assume smoothness and let the data tell me the shape."</em> It's a powerful non-parametric model that gives both predictions <strong>and confidence intervals</strong> in a mathematically elegant way.</p>

          <h3>3. Bayesian Neural Networks</h3>
          <p>Neural networks, but probabilistic: instead of fixed weights, we learn <strong>distributions over weights</strong>. This lets the network say <em>"I'm confident here"</em> or <em>"I'm uncertain there"</em>, which is critical for safe decision-making in the real world.</p>

          <h2> Part II: Uncertainty in Sequential Tasks</h2>

          <p>The second part is about uncertainty in sequential tasks. We consider active learning and optimization approaches that actively collect data by proposing experiments that are <strong>informative</strong> for reducing epistemic uncertainty.</p>

          <h3> 4. Bayesian Optimization</h3>
          <p>When experiments are expensive (think: running clinical trials or tuning giant ML models), Bayesian optimization helps choose the <strong>next experiment to run</strong> by balancing exploration and exploitation. It actively seeks the most <em>informative</em> data points to shrink uncertainty fastest.</p>

          <h3> How Do We Know Which Experiments Are Informative?</h3>
          <p>Great question! We measure informativeness through <strong>epistemic uncertainty</strong> ‚Äî the uncertainty that comes from not having enough data. By asking <em>"where am I most ignorant?"</em>, the algorithm proposes experiments that are maximally clarifying.</p>

          <p>When we say an algorithm knows which experiments are informative, we're talking about <strong>information-theoretic criteria</strong> that measure how much an experiment would reduce <em>epistemic uncertainty</em>:</p>

          <ul>
            <li><strong>Posterior variance</strong>: In Gaussian processes, uncertainty at a point is quantified by the posterior variance. The algorithm chooses new inputs where this variance is largest.</li>
            <li><strong>Expected information gain</strong>: <em>"If I ran this experiment, how much would it shrink my uncertainty about the model?"</em> This is formalized as maximizing the expected reduction in entropy.</li>
            <li><strong>Acquisition functions</strong>: Functions like <strong>Upper Confidence Bound (UCB)</strong> or <strong>Expected Improvement (EI)</strong> trade off exploration and exploitation:
              <ul>
                <li>UCB: <em>"Try points with high predicted value plus high uncertainty."</em></li>
                <li>EI: <em>"Try points likely to improve upon the best outcome so far."</em></li>
              </ul>
            </li>
          </ul>

          <h2> Reinforcement Learning & MDPs</h2>

          <p>Then we cover reinforcement learning (RL), a rich formalism for modeling agents that learn to act in uncertain environments.</p>

          <h3> 5. Markov Decision Process (MDP)</h3>
          <p>The MDP is the mathematical backbone of reinforcement learning. It models the world as states, actions, and rewards, capturing the idea that decisions today affect both what you see tomorrow and what long-term payoff you'll get.</p>

          <h3> 6. RL with Neural Network Approximations</h3>
          <p>Modern RL uses deep neural networks to approximate value functions or policies in huge state spaces. This gives us "deep RL" ‚Äî the engine behind agents that can play Go, control robots, or learn strategies in complex, high-dimensional environments.</p>

          <h2> Model-Based RL and Safety</h2>

          <p>We close by discussing modern approaches in model-based RL, which use epistemic and aleatoric uncertainty to guide exploration, while also reasoning about safety.</p>

          <blockquote>
            <p><strong>The Big Picture:</strong> Probabilistic AI is not just about making machines <em>smart</em>, but about making them <em>humble</em>: systems that <strong>know what they don't know</strong>, and act cautiously when uncertainty is high. That's the difference between a reckless model and a trustworthy intelligent agent.</p>
          </blockquote>

          <h2> Why This Matters</h2>

          <p>In a world where AI systems are making increasingly important decisions ‚Äî from medical diagnoses to autonomous vehicle control ‚Äî the ability to quantify and reason about uncertainty isn't just mathematically elegant, it's <strong>ethically essential</strong>.</p>

          <p>PAI gives us the tools to build AI systems that:</p>
          <ul>
            <li>Make better decisions under uncertainty</li>
            <li>Know when to ask for more data</li>
            <li>Fail safely when confidence is low</li>
            <li>Learn more efficiently by being strategic about what to explore</li>
          </ul>

          <p>This course is reshaping how I think about intelligence itself ‚Äî not just as the ability to be right, but as the wisdom to know when you might be wrong.</p>

          <h2> Completed Tasks & Projects</h2>

          <p>Below are the key tasks I've completed for the PAI exam, demonstrating practical applications of the theoretical concepts covered in the course.</p>

          <div class="task-container">
            <h3> Task 1: Bayesian Linear Regression with Model Selection</h3>
            
            <div class="task-section">
              <h4>The Problem</h4>
              <p>Given a dataset with input-output pairs, we need to:</p>
              <ul>
                <li>Implement <strong>Bayesian Linear Regression</strong> to model the relationship between inputs and outputs</li>
                <li>Perform <strong>model selection</strong> by comparing different polynomial feature transformations</li>
                <li>Use the <strong>marginal likelihood (evidence)</strong> to select the best model complexity</li>
                <li>Quantify prediction uncertainty using the posterior predictive distribution</li>
              </ul>
            </div>

            <div class="task-section">
              <h4>Our Approach</h4>
              <p>We tackled this problem using a fully Bayesian framework:</p>
              <ul>
                <li><strong>Feature Engineering:</strong> Applied polynomial transformations of varying degrees (1 to 10) to capture non-linear relationships</li>
                <li><strong>Bayesian Inference:</strong> Instead of point estimates, we computed full posterior distributions over model parameters using conjugate priors (Gaussian-Gamma)</li>
                <li><strong>Model Evidence:</strong> Calculated the marginal likelihood for each polynomial degree, which naturally penalizes overfitting through the Bayesian Occam's Razor principle</li>
                <li><strong>Posterior Predictive:</strong> Generated predictions with credible intervals that reflect both aleatoric (data noise) and epistemic (parameter) uncertainty</li>
              </ul>
            </div>

            <div class="task-section">
              <h4>Key Insights</h4>
              <ul>
                <li> <strong>Marginal Likelihood as Model Selection:</strong> The evidence automatically trades off model fit and complexity, selecting simpler models when data doesn't justify complexity</li>
                <li> <strong>Uncertainty Quantification:</strong> Unlike traditional regression, Bayesian methods provide confidence intervals that widen in regions with sparse data</li>
                <li> <strong>Occam's Razor in Action:</strong> More complex models are penalized unless they significantly improve fit, preventing overfitting</li>
                <li> <strong>No Cross-Validation Needed:</strong> The marginal likelihood handles model selection in one pass, without requiring train/validation splits</li>
              </ul>
            </div>
          </div>

          <div class="task-container">
            <h3> Task 2: Gaussian Processes for Regression</h3>
            
            <div class="task-section">
              <h4>The Problem</h4>
              <p>Implement Gaussian Process (GP) regression to model complex, non-linear functions with uncertainty quantification:</p>
              <ul>
                <li>Build a <strong>Gaussian Process model</strong> from scratch using kernel functions</li>
                <li>Handle <strong>noisy observations</strong> while maintaining smooth predictions</li>
                <li>Implement different <strong>kernel functions</strong> (RBF, Mat√©rn, Periodic) to capture various function behaviors</li>
                <li>Optimize <strong>hyperparameters</strong> (length scale, variance, noise) using maximum likelihood estimation</li>
                <li>Provide <strong>uncertainty bounds</strong> that adapt to data density</li>
              </ul>
            </div>

            <div class="task-section">
              <h4>Our Approach</h4>
              <p>We implemented a complete Gaussian Process regression framework:</p>
              <ul>
                <li><strong>Kernel Design:</strong> Implemented multiple kernel functions including RBF (Radial Basis Function), Mat√©rn, and Periodic kernels to model different smoothness and periodicity assumptions</li>
                <li><strong>GP Posterior Computation:</strong> Derived and implemented the exact posterior distribution over functions given observed data, using the kernel trick to avoid explicit feature space computation</li>
                <li><strong>Hyperparameter Optimization:</strong> Maximized the log marginal likelihood with respect to kernel hyperparameters using gradient-based optimization (e.g., L-BFGS)</li>
                <li><strong>Numerical Stability:</strong> Applied Cholesky decomposition for efficient and numerically stable inversion of covariance matrices</li>
                <li><strong>Predictive Distribution:</strong> Computed mean and variance of the posterior predictive distribution at test points, providing both point predictions and uncertainty estimates</li>
              </ul>
            </div>

            <div class="task-section">
              <h4>Key Insights</h4>
              <ul>
                <li> <strong>Non-Parametric Flexibility:</strong> GPs define distributions over functions rather than parameters, allowing them to adapt to arbitrary function complexity without fixed model structure</li>
                <li> <strong>Kernel as Prior:</strong> The choice of kernel encodes our prior beliefs about function smoothness, periodicity, and correlation structure‚Äîcritical for good predictions</li>
                <li> <strong>Uncertainty-Aware Predictions:</strong> Predictive variance naturally increases in regions far from training data, providing honest uncertainty quantification</li>
                <li> <strong>Computational Challenges:</strong> GP inference scales as O(n¬≥) due to matrix inversion, requiring approximations (e.g., inducing points) for large datasets</li>
                <li> <strong>Automatic Relevance Determination:</strong> Hyperparameter optimization naturally performs feature selection by learning which input dimensions are most relevant</li>
                <li> <strong>Connection to Bayesian Linear Regression:</strong> GPs can be seen as infinite-dimensional extensions of Bayesian linear regression with infinite basis functions</li>
              </ul>
            </div>
          </div>

          <div class="task-container">
            <h3> Task 3: Safe Bayesian Optimization with Constraints</h3>
            
            <div class="task-section">
              <h4>The Problem</h4>
              <p>Optimize a black-box objective function while respecting safety constraints‚Äîa critical challenge in real-world applications like drug design:</p>
              <ul>
                <li>Maximize <strong>bioavailability f(x)</strong> of a drug formulation</li>
                <li>Ensure <strong>surface area v(x) ‚â§ 4.0</strong> to remain safe for patients</li>
                <li>Handle <strong>noisy observations</strong> from expensive experiments</li>
                <li>Never violate safety constraints during the optimization process (<strong>safe exploration</strong>)</li>
                <li>Balance <strong>exploration vs. exploitation</strong> within the safe set</li>
              </ul>
              <p>The key challenge: we cannot afford to test unsafe configurations, so we must learn both the objective and constraint functions conservatively.</p>
            </div>

            <div class="task-section">
              <h4>Our Approach</h4>
              <p>We implemented a dual-GP safe Bayesian optimization framework:</p>
              <ul>
                <li><strong>Dual Gaussian Processes:</strong> Maintained two independent GPs‚Äîone modeling the objective f(x) and another modeling the constraint violation g(x) = v(x) - 4.0</li>
                <li><strong>Conservative Safe Set:</strong> Defined a point as safe if Œº_g(x) + Œ≤¬∑œÉ_g(x) ‚â§ -Œµ, where Œ≤ = 3.0 provides high-probability safety guarantees (analogous to UCB but for constraints)</li>
                <li><strong>Kernel Engineering:</strong> Used Mat√©rn kernels for f(x) with fixed noise (œÉ_f = 0.15) and a composite kernel (Linear + Mat√©rn + RBF) for v(x) to capture both global trends and local variations</li>
                <li><strong>Safe Expected Improvement:</strong> Maximized Expected Improvement (EI) acquisition function only over the safe set, ensuring all proposed experiments respect safety constraints</li>
                <li><strong>Exploration Bonus:</strong> Added a small variance term to the acquisition to avoid premature convergence and encourage safe exploration</li>
                <li><strong>Graceful Degradation:</strong> Implemented fallback mechanisms when the GP becomes overly pessimistic‚Äîre-sample near previously safe observations</li>
              </ul>
            </div>

            <div class="task-section">
              <h4>Key Insights</h4>
              <ul>
                <li> <strong>Safety Through Uncertainty:</strong> High uncertainty in constraint predictions forces conservative behavior‚Äîthe algorithm won't risk unsafe regions until it has more data</li>
                <li> <strong>Exploration-Safety Tradeoff:</strong> Safe BO must balance three objectives: maximize reward, explore the space, and never violate constraints‚Äîa harder problem than unconstrained BO</li>
                <li> <strong>Dual Modeling:</strong> Separate GPs for objective and constraint allow independent uncertainty quantification, crucial since we need conservative constraint estimates but optimistic objective estimates</li>
                <li> <strong>Never Leave the Safe Set:</strong> Unlike penalty methods or barrier functions, safe BO provides hard guarantees: if the initial point is safe and the GP is well-calibrated, all queries remain safe with high probability</li>
                <li> <strong>Beta Parameter Tuning:</strong> The safety parameter Œ≤ controls risk aversion‚Äîhigher Œ≤ means more conservative (larger safe set margin), lower Œ≤ allows more aggressive exploration</li>
                <li> <strong>Real-World Applications:</strong> This framework is critical for domains where constraint violations are unacceptable: medical treatments, autonomous systems, chemical processes, robotics</li>
                <li> <strong>Sample Efficiency:</strong> Safe BO achieves near-optimal solutions in fewer evaluations than random search or grid search while maintaining safety, crucial when experiments are expensive or time-consuming</li>
              </ul>
            </div>
          </div>

          <div class="task-container">
            <h3> Task 4: Maximum a Posteriori Policy Optimization (MPO)</h3>
            
            <div class="task-section">
              <h4>The Problem</h4>
              <p>Implement a state-of-the-art deep reinforcement learning algorithm for continuous control:</p>
              <ul>
                <li>Train an agent to solve the <strong>CartPole-v1</strong> control task using only raw observations</li>
                <li>Handle <strong>continuous action spaces</strong> with Gaussian stochastic policies</li>
                <li>Balance <strong>exploration and exploitation</strong> while learning from experience</li>
                <li>Ensure <strong>stable learning</strong> by constraining policy updates to avoid catastrophic forgetting</li>
                <li>Use <strong>off-policy learning</strong> with a replay buffer for sample efficiency</li>
                <li>Implement <strong>actor-critic architecture</strong> with twin Q-functions for value estimation</li>
              </ul>
            </div>

            <div class="task-section">
              <h4>Our Approach</h4>
              <p>We implemented MPO, an EM-style algorithm that constrains policy updates using KL divergence:</p>
              <ul>
                <li><strong>Twin Critics (Clipped Double Q-Learning):</strong> Maintained two independent Q-networks (Q‚ÇÅ, Q‚ÇÇ) and used their minimum for target computation to reduce overestimation bias</li>
                <li><strong>Gaussian Stochastic Actor:</strong> Policy network outputs mean Œº and log-std for each action dimension, creating a diagonal Gaussian distribution. Actions are sampled using the reparameterization trick and squashed through tanh for bounded control</li>
                <li><strong>E-Step (Critic Update):</strong> Updated critics toward Bellman targets computed by sampling K actions from the target policy at next states and averaging their Q-values</li>
                <li><strong>M-Step (Policy Update):</strong> Updated policy to maximize expected Q-value using importance-weighted maximum likelihood:
                  <ul>
                    <li>Sample K actions from current policy at observed states</li>
                    <li>Compute importance weights w_k ‚àù exp(Q(s,a_k)/Œ∑) using softmax</li>
                    <li>Maximize weighted log-likelihood: Œ£_k w_k log œÄ(a_k|s)</li>
                  </ul>
                </li>
                <li><strong>Temperature Parameter Œ∑:</strong> Adaptively controlled via Lagrangian optimization to maintain KL(œÄ_old || œÄ_new) ‚âà Œµ_KL, preventing drastic policy changes</li>
                <li><strong>Soft Target Updates:</strong> Applied Polyak averaging (œÑ=0.005) to slowly update target networks, stabilizing learning</li>
                <li><strong>Replay Buffer:</strong> Stored 50k transitions for off-policy sampling, enabling data reuse and breaking temporal correlations</li>
              </ul>
            </div>

            <div class="task-section">
              <h4>Key Insights</h4>
              <ul>
                <li> <strong>EM Perspective on RL:</strong> MPO frames policy improvement as an EM algorithm‚ÄîE-step evaluates actions, M-step improves policy toward high-value actions weighted by their advantage</li>
                <li> <strong>Trust Region via KL Constraint:</strong> Unlike PPO's clipping or TRPO's line search, MPO uses a temperature parameter to implicitly enforce trust regions, ensuring smooth policy evolution</li>
                <li> <strong>Sample Reweighting:</strong> Importance weights naturally implement advantage-weighted regression: actions with high Q-values get more influence on policy updates</li>
                <li> <strong>Continuous Control Challenges:</strong> Tanh squashing ensures actions stay within bounds while maintaining differentiability for gradient-based optimization</li>
                <li> <strong>Twin Q-Functions:</strong> Taking min(Q‚ÇÅ, Q‚ÇÇ) for target computation reduces overoptimistic Q-value estimates that plague single-critic methods, improving stability</li>
                <li> <strong>Off-Policy Efficiency:</strong> Replay buffer enables multiple gradient updates per environment step, dramatically improving sample efficiency compared to on-policy methods like PPO</li>
                <li> <strong>Reparameterization Trick:</strong> Sampling a ~ N(Œº, œÉ) as a = Œº + œÉ¬∑Œµ (Œµ ~ N(0,1)) allows backpropagation through stochastic sampling, critical for policy gradient estimation</li>
                <li> <strong>Stabilization Techniques:</strong> Combination of target networks, twin critics, soft updates, and KL constraints creates a remarkably stable learning algorithm suitable for complex control tasks</li>
              </ul>
            </div>
          </div>

        </div>
        
        <div class="post-tags">
          <span class="tag">Probabilistic AI</span>
          <span class="tag">Bayesian Methods</span>
          <span class="tag">Uncertainty Quantification</span>
          <span class="tag">ETH Zurich</span>
          <span class="tag">Machine Learning</span>
          <span class="tag">Reinforcement Learning</span>
        </div>
      </article>
    </main>
  </div>

  <footer class="site-footer">
    <div class="footer-content">
      <p>&copy; 2026 Gennaro Francesco Landi. All rights reserved.</p>
      <div class="footer-links">
        <a href="https://github.com/landigf" target="_blank"><i class="fab fa-github"></i></a>
        <a href="https://www.linkedin.com/in/landigf" target="_blank"><i class="fab fa-linkedin"></i></a>
      </div>
    </div>
  </footer>

  <script>
    // Dark mode toggle functionality
    document.addEventListener('DOMContentLoaded', function() {
      const themeToggle = document.getElementById('theme-toggle');
      const themeIcon = themeToggle.querySelector('i');
      
      // Check for saved theme preference
      const savedTheme = localStorage.getItem('theme');
      if (savedTheme) {
        document.body.classList.add(savedTheme);
        if (savedTheme === 'dark-mode') {
          themeIcon.classList.replace('fa-moon', 'fa-sun');
        }
      }
      
      themeToggle.addEventListener('click', function() {
        document.body.classList.toggle('dark-mode');
        
        if (document.body.classList.contains('dark-mode')) {
          themeIcon.classList.replace('fa-moon', 'fa-sun');
          localStorage.setItem('theme', 'dark-mode');
        } else {
          themeIcon.classList.replace('fa-sun', 'fa-moon');
          localStorage.setItem('theme', 'light-mode');
        }
      });
    });
  </script>
</body>
</html>